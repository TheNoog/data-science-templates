# Hidden Markov Models

A Hidden Markov Model (HMM) is a statistical model used to describe and analyze systems that are assumed to be Markov processes with unobserved (hidden) states. It is widely used in various fields, including speech recognition, natural language processing, bioinformatics, and finance.

In an HMM, we have a sequence of observations, and the goal is to infer the underlying sequence of hidden states that generated these observations. The HMM assumes that the underlying states form a Markov chain, where the probability of transitioning to a particular state depends only on the previous state.

The basic components of an HMM are:

    Hidden States: These are the unobserved states that generate the observations. Each hidden state represents a specific condition or situation in the modeled system.

    Observations: These are the visible outcomes or measurements that we can directly observe. Each observation is generated by a hidden state.

    Transition Probabilities: These represent the probabilities of transitioning between hidden states. The probability of moving from one hidden state to another is determined solely by the current state and does not depend on previous states.

    Emission Probabilities: These represent the probabilities of observing a particular outcome or measurement given a hidden state. Each hidden state has a probability distribution over possible observations.

The HMM operates in two fundamental steps:

    Learning (Training): Given a set of observed sequences, the HMM can be trained to estimate the model parameters, including the initial state probabilities, transition probabilities, and emission probabilities. This process is often performed using the Baum-Welch algorithm (also known as the forward-backward algorithm) or other similar methods.

    Inference (Decoding): Once the model is trained, it can be used for inference tasks. Given a new sequence of observations, the HMM can compute the most likely sequence of hidden states that generated those observations. This is typically done using the Viterbi algorithm, which efficiently finds the optimal hidden state sequence based on the model parameters.

By utilizing the underlying Markov assumption and probabilistic calculations, HMMs can capture temporal dependencies and model complex systems with hidden information. They provide a powerful framework for sequence analysis and prediction tasks.

<br />

## Example

Let's consider a simple example of a weather forecasting system using a Hidden Markov Model (HMM). In this example, we want to predict the weather (hidden states) based on the observed activities (observations) throughout the day.

Assume we have three possible weather conditions: sunny, cloudy, and rainy. The observed activities could be "walk," "shop," or "clean." We want to determine the most likely sequence of weather conditions that generated these activities.

Here are the components of the HMM for this example:

    Hidden States (Weather Conditions):
        S₁: Sunny
        S₂: Cloudy
        S₃: Rainy

    Observations (Activities):
        O₁: Walk
        O₂: Shop
        O₃: Clean

    Transition Probabilities (A):
        A = [aᵢⱼ], where aᵢⱼ represents the probability of transitioning from state Sᵢ to state Sⱼ. For example:
            a₁₁: Probability of staying sunny given it was sunny the previous day
            a₁₂: Probability of transitioning from sunny to cloudy
            a₁₃: Probability of transitioning from sunny to rainy
            and so on...

    Emission Probabilities (B):
        B = [bᵢⱼ], where bᵢⱼ represents the probability of emitting observation Oⱼ given the state Sᵢ. For example:
            b₁₁: Probability of observing a walk when it's sunny
            b₁₂: Probability of observing a shop when it's sunny
            b₁₃: Probability of observing a clean activity when it's sunny
            and so on...

To make predictions using the HMM, we apply the Viterbi algorithm. Given a sequence of observations (e.g., [Walk, Shop, Clean]), we want to find the most likely sequence of hidden states (weather conditions).

The Viterbi algorithm calculates the most probable path by recursively computing two probabilities for each state at each time step:

    Forward Probability (Viterbi variable):
        Vₜᵢ: The probability of the most probable sequence of hidden states up to time t that ends in state Sᵢ and produces the observed sequence up to time t.

    Backpointer:
        Bₜᵢ: The previous state in the most probable sequence that leads to state Sᵢ at time t.

Using these probabilities, we can calculate the Viterbi variables and backpointers for each time step, and then backtrack to find the most likely sequence of hidden states.

The formula for calculating the Viterbi variables (forward probabilities) is as follows:

Vₜᵢ = maxₛ₍ₑ₎ (Vₜ₋₁ₛ * aₛᵢ * bᵢ(Oₜ))

where:

    t: current time step
    i: current state
    s: previous state
    e: all possible previous states
    Vₜ₋₁ₛ: Viterbi variable for the previous time step (t-1) and previous state (s)
    aₛᵢ: transition probability from state s to state i
    bᵢ(Oₜ): emission probability of observation Oₜ given state i

By computing these forward probabilities and backpointers, we can find the most likely sequence of hidden states.

This is just a high-level explanation of the HMM and the Viterbi algorithm using a simple example. In practice, HMMs can have more complex state transitions and emission probabilities, and more advanced algorithms are used for training and inference.