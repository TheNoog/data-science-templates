# Markov Chain Monte Carlo

Markov Chain Monte Carlo (MCMC) is a computational algorithm used to generate samples from a complex probability distribution. It is particularly useful when direct sampling or analytical calculation of the distribution is difficult or infeasible. MCMC is widely used in various fields such as statistics, physics, machine learning, and Bayesian inference.

The basic idea behind MCMC is to construct a Markov chain that has the desired probability distribution as its stationary distribution. The Markov chain is defined by a set of transition probabilities, which determine the probability of moving from one state to another. At each iteration of the algorithm, a proposal state is generated based on the current state, and the decision to accept or reject the proposal is made based on a criterion that preserves the desired distribution.

The key property of MCMC is that, as the algorithm progresses, the Markov chain converges to its stationary distribution, which is the desired probability distribution. The samples generated by MCMC can then be used to approximate various properties of the target distribution, such as the mean, variance, or other moments.

One commonly used variant of MCMC is the Metropolis-Hastings algorithm. It generates proposals based on a proposal distribution and uses an acceptance probability to decide whether to accept or reject the proposal. The acceptance probability is calculated based on the ratio of the target distribution's probability at the proposed state to the probability at the current state.

MCMC methods are particularly useful in Bayesian inference, where the goal is to estimate the posterior distribution of model parameters given observed data. By sampling from the posterior distribution using MCMC, it becomes possible to perform inference, make predictions, and quantify uncertainty.

It's important to note that MCMC is not without limitations. It can be computationally expensive and may require careful tuning of parameters for optimal performance. Additionally, convergence to the stationary distribution may be slow for certain complex distributions or when the Markov chain has poor mixing properties. Nonetheless, MCMC remains a valuable tool for probabilistic modeling and inference in many practical scenarios.

The Metropolis-Hastings algorithm is a Markov Chain Monte Carlo (MCMC) algorithm used for sampling from complex probability distributions. It provides a general framework for generating samples from a target distribution when direct sampling or analytical calculation of the distribution is difficult.

The algorithm is based on constructing a Markov chain in which each state represents a potential sample from the target distribution. At each iteration of the algorithm, a proposal state is generated based on the current state, and the decision to accept or reject the proposal is made based on an acceptance probability.

The steps of the Metropolis-Hastings algorithm are as follows:

    Start with an initial state.
    Generate a proposal state from a proposal distribution. The proposal distribution defines how new candidate states are generated based on the current state.
    Calculate the acceptance probability, which determines the probability of accepting the proposal and transitioning to the new state. The acceptance probability is typically calculated as the ratio of the target distribution's probability at the proposed state to the probability at the current state, multiplied by the ratio of the proposal distributions' probabilities for transitioning from the current state to the proposed state and vice versa.
    Accept or reject the proposal based on the acceptance probability. If the proposal is accepted, move to the proposed state; otherwise, stay at the current state.
    Repeat steps 2-4 for a predetermined number of iterations.

By repeating these steps, the Markov chain explores the space of potential samples, with a higher probability of visiting states that have higher probability under the target distribution. Over time, the Markov chain converges to its stationary distribution, which is the desired target distribution.

The Metropolis-Hastings algorithm provides a way to sample from complex distributions by using a proposal distribution that may be simpler to sample from or evaluate. The acceptance step allows for the exploration of the target distribution while ensuring detailed balance, which guarantees that the Markov chain converges to the correct distribution.

The algorithm can be applied to a wide range of problems and is particularly useful in Bayesian inference, where the goal is to sample from the posterior distribution of model parameters given observed data. By using the Metropolis-Hastings algorithm, it becomes possible to approximate the posterior distribution and perform inference even when direct sampling or analytical calculation is infeasible.